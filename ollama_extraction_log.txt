================================================================================
KNOWLEDGE GRAPH EXTRACTION - SINGLE TEXT TEST (Ollama)
Using LangExtract for: Source Grounding, Few-shot Learning, Long Doc Optimization
================================================================================
Model Provider: ollama
Model Name: gemma3:12b
Ollama URL: http://localhost:11434/v1
Input JSON: /app/data/legal/processed/legal_background.jsonl
Record ID: UKSC-2009-0143
Text Field: text
Output Directory: /app/test_outputs/single_extraction_ollama_20260110_163814

LangExtract Configuration:
  â€¢ Extraction passes: 1
  â€¢ Max workers: 5
  â€¢ Max char buffer: 8000

Extraction Method: Two-Step Connectivity-Aware
  â€¢ Max disconnected: 1
  â€¢ Max iterations: 5
================================================================================

Checking Ollama connection...
âœ“ Ollama is reachable at http://localhost:11434/v1
  Available models: ['gemma3:12b']

Running extraction pipeline with LangExtract (Ollama)...


================================================================================
STEP 1: LOADING INPUT DATA
================================================================================
Loading JSONL file: /app/data/legal/processed/legal_background.jsonl
âœ“ Found record at line 1
âœ“ Loaded record: UKSC-2009-0143
âœ“ Text field: text
âœ“ Text length: 1697 characters

Text preview (first 200 chars):
Sigma Finance Corporation is a structured investment vehicle (SIV) established to invest in certain types of asset-backed securities and other financ ial instruments. Sigma aimed to profit from the di...

================================================================================
STEP 2: INITIALIZING EXTRACTION PIPELINE (WITH LANGEXTRACT + LM STUDIO)
================================================================================
Using two-step extraction prompts:
  â€¢ Step 1 (initial): /app/src/kg_constructor/prompts/legal_background_prompt_step1_initial.txt
  â€¢ Step 2 (bridging): /app/src/kg_constructor/prompts/legal_background_prompt_step2_bridging.txt
âœ“ Client: OllamaClient
âœ“ Model: ollama/gemma3:12b
âœ“ Ollama URL: http://localhost:11434/v1
âœ“ Temperature: 0.0

LangExtract Features Enabled:
  âœ“ Source Grounding (char_start, char_end for each triple)
  âœ“ Few-shot Examples (guiding extraction quality)
  âœ“ Controlled Generation (JSON schema constraints)
  âœ“ Long Document Optimization (passes=1, workers=5)

================================================================================
STEP 3: EXTRACTING TRIPLES WITH LANGEXTRACT (LM STUDIO)
================================================================================
Using Two-Step Connectivity-Aware Extraction
  â€¢ Max disconnected components: 1
  â€¢ Max iterations: 5

WARNING:absl:Prompt alignment: non-exact match: [example#0] class='Triple' status=AlignmentStatus.MATCH_FUZZY text='John Smith ... senior software engineer' char_span=(0, 61)
WARNING:absl:Prompt alignment: non-exact match: [example#2] class='Triple' status=AlignmentStatus.MATCH_FUZZY text='Sarah Johnson ... represents the plaintiff' char_span=(0, 63)
[94m[1mLangExtract[0m: Processing [00:00][94m[1mLangExtract[0m: Processing [00:00]
Traceback (most recent call last):
  File "/app/src/kg_constructor/clients/ollama_client.py", line 103, in extract
    result = lx.extract(
             ^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/__init__.py", line 55, in extract
    return extract_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/extraction.py", line 330, in extract
    result = annotator.annotate_text(
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/annotation.py", line 560, in annotate_text
    annotations = list(
                  ^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/annotation.py", line 255, in annotate_documents
    yield from self._annotate_documents_single_pass(
  File "/usr/local/lib/python3.11/site-packages/langextract/annotation.py", line 353, in _annotate_documents_single_pass
    prompts = [
              ^
  File "/usr/local/lib/python3.11/site-packages/langextract/annotation.py", line 354, in <listcomp>
    self._prompt_generator.render(
  File "/usr/local/lib/python3.11/site-packages/langextract/prompting.py", line 133, in render
    prompt_lines.append(self.format_example_as_text(ex))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/prompting.py", line 107, in format_example_as_text
    answer = self.format_handler.format_extraction_example(example.extractions)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/core/format_handler.py", line 147, in format_extraction_example
    return self._add_fences(formatted) if self.use_fences else formatted
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/langextract/core/format_handler.py", line 249, in _add_fences
    fence_type = self.format_type.value
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py", line 289, in __getattr__
    raise AttributeError(item)
AttributeError: value

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/test_outputs/single_extraction_ollama_20260110_163814/extract_single.py", line 456, in <module>
    main()
  File "/app/test_outputs/single_extraction_ollama_20260110_163814/extract_single.py", line 164, in main
    triples, metadata = pipeline.extractor.extract_connected_graph(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/kg_constructor/extractor.py", line 317, in extract_connected_graph
    triples = self.extract_from_text(text, record_id, temperature, max_tokens)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/kg_constructor/extractor.py", line 197, in extract_from_text
    raw_triples = self.client.extract(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/app/src/kg_constructor/clients/ollama_client.py", line 136, in extract
    raise LLMClientError(f"Ollama extraction failed: {e}") from e
kg_constructor.clients.base.LLMClientError: Ollama extraction failed: value
